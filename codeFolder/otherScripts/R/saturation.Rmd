---
title: "for_david"
author: "Nik"
date: "14/03/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

\section{A quick computation for multinomial entrop}

We have to find a fast way to calculate the entropy H(X) = E(I(X)) for X a multinomial distribution. We will use 

$$ H(X) = -\log n! - n \sum_{i=0}^k p_i \log(pi) + \sum_{i=0}^k \sum_{x_i=0}^n {n \choose x_i} p_i^{x_i}(1-p_i)^{n - x_i} \log x_i ! $$

Which is $O((n+1)k)$. A more rapid version can be obtained by using Stirling's approximation, but it's not exact.

```{r}
calc_term <- function(n, p_i){ # for a single p_i calculates the term inside the sum of the p_i
  
  nvec = seq(1, n, 1)
  new = choose(n,nvec)*p_i^(nvec)*((1-p_i)^(n -nvec))*log2(factorial(nvec))
  print(new)
  return(sum(new))
}

multinomial_entropy <- function(n, p){ # n the number of lines in sequence, p a vector of probabilities for multinomial distribution
  
  term_3 = sum(calc_term(p)) 
  
  calc <- -log2(factorial(n)) - n*sum(p*log2(p)) + term_3
  
  
  return(calc)
}
```

\section{Application}

Now we want to apply this to a file of sequence data.

The idea is that at each site we will calculate the information of that site, sum across all informations to obtain an estimate of the entropy $ \hat{H}$.

Then we can compare $\hat{H}$ to $H*$, the entropy we expect under saturation.

That means we first need to define a function for the information content of an observation. The information is given by:

$$ I(X|X = x) = I(x) = - \log_2 (P(X = x))  $$

```{r}
### calculates the self-information of an observation x. In this case, x is the vector of counts of each nucleotide. p is the vector of probabilities of each draw.
### p_i should be the overall frequency of each base i in the alignment 
multinomial_info <- function(x, p){ 
 P_x = dmultinom(x, prob = p)
 

  return(- log2(P_x))
}
```

\subsection{Data Wrangling}
Flow: take sequence data, wrangle it into sites.

\subsection{Analysis}

Then -> compute base frequencies.
for each site, compute information
compute overall entropy
divide and done

\section{Test Code}


##### Make test cases #####
